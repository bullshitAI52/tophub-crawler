"""
ç›´æ¥è¯·æ±‚çˆ¬è™«
ä½¿ç”¨requestsåº“ç›´æ¥è¯·æ±‚
"""

import requests
import random
import re
from typing import Optional, List
from bs4 import BeautifulSoup

from .base_crawler import BaseCrawler, HotItem
from config.settings import (
    USER_AGENTS, HEADERS_TEMPLATE, REQUEST_TIMEOUT,
    MAX_RETRIES, DEBUG
)


class TophubDirectCrawler(BaseCrawler):
    """ç›´æ¥è¯·æ±‚çˆ¬è™«"""
    
    def __init__(self, debug: bool = DEBUG):
        super().__init__(debug)
        self.session = requests.Session()
        self._setup_session()
    
    def _setup_session(self):
        """è®¾ç½®ä¼šè¯"""
        # åŸºç¡€è¯·æ±‚å¤´
        headers = HEADERS_TEMPLATE.copy()
        headers['User-Agent'] = random.choice(USER_AGENTS)
        self.session.headers.update(headers)
        
        # å…¶ä»–è®¾ç½®
        self.session.max_redirects = 5
    
    def _rotate_user_agent(self):
        """è½®æ¢ç”¨æˆ·ä»£ç†"""
        self.session.headers['User-Agent'] = random.choice(USER_AGENTS)
    
    def fetch_page(self, url: str) -> Optional[str]:
        """è·å–é¡µé¢å†…å®¹"""
        for attempt in range(MAX_RETRIES):
            try:
                # è½®æ¢ç”¨æˆ·ä»£ç†
                self._rotate_user_agent()
                
                # è®¾ç½®Referer
                if attempt == 0:
                    self.session.headers['Referer'] = self.base_url
                else:
                    self.session.headers['Referer'] = url
                
                self._debug_log(f"å°è¯• {attempt+1}/{MAX_RETRIES}: {url}")
                
                response = self.session.get(
                    url,
                    timeout=REQUEST_TIMEOUT,
                    allow_redirects=True
                )
                
                # æ£€æŸ¥çŠ¶æ€ç 
                if response.status_code == 200:
                    self._debug_log(f"è¯·æ±‚æˆåŠŸ: {url}")
                    return response.text
                elif response.status_code == 403:
                    self._debug_log(f"403 Forbidden: {url}")
                elif response.status_code == 404:
                    self._debug_log(f"404 Not Found: {url}")
                    return None
                elif response.status_code == 429:
                    self._debug_log(f"429 Too Many Requests: {url}")
                    # ç­‰å¾…æ›´é•¿æ—¶é—´
                    if attempt < MAX_RETRIES - 1:
                        wait_time = 5 * (attempt + 1)
                        self._debug_log(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•")
                        time.sleep(wait_time)
                else:
                    self._debug_log(f"çŠ¶æ€ç  {response.status_code}: {url}")
                
            except requests.exceptions.Timeout:
                self._debug_log(f"è¯·æ±‚è¶…æ—¶: {url}")
            except requests.exceptions.ConnectionError:
                self._debug_log(f"è¿æ¥é”™è¯¯: {url}")
            except Exception as e:
                self._debug_log(f"è¯·æ±‚å¼‚å¸¸: {e}")
            
            # é‡è¯•å‰å»¶è¿Ÿ
            if attempt < MAX_RETRIES - 1:
                retry_delay = 2 * (attempt + 1)
                self._debug_log(f"ç­‰å¾… {retry_delay} ç§’åé‡è¯•")
                time.sleep(retry_delay)
        
        self._log(f"æ‰€æœ‰é‡è¯•å¤±è´¥: {url}", "ERROR")
        return None
    
    def parse_hot_items(self, html: str, platform: str) -> List[HotItem]:
        """è§£æçƒ­ç‚¹æ•°æ®"""
        from datetime import datetime
        
        soup = BeautifulSoup(html, 'html.parser')
        items = []
        
        # å¹³å°ç‰¹å®šçš„è§£æé€»è¾‘
        if platform == 'weibo':
            items = self._parse_weibo(soup)
        elif platform == 'zhihu':
            items = self._parse_zhihu(soup)
        elif platform == 'baidu':
            items = self._parse_baidu(soup)
        else:
            # é€šç”¨è§£æ
            items = self._parse_general(soup)
        
        # æ·»åŠ æ—¶é—´æˆ³
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        for item in items:
            item.timestamp = timestamp
            item.category = platform
        
        return items
    
    def _parse_weibo(self, soup: BeautifulSoup) -> List[HotItem]:
        """è§£æå¾®åšçƒ­æœ"""
        items = []
        
        # å°è¯•ä¸åŒçš„é€‰æ‹©å™¨
        selectors = [
            "table tbody tr",
            ".list .item",
            ".rank-list li",
            "tr[class*='item']",
        ]
        
        for selector in selectors:
            elements = soup.select(selector)
            if len(elements) > 5:
                self._debug_log(f"ä½¿ç”¨é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ° {len(elements)} ä¸ªå…ƒç´ ")
                
                for idx, element in enumerate(elements[:50], 1):
                    try:
                        # æå–æ–‡æœ¬
                        text = element.get_text(strip=True)
                        if not text or len(text) < 5:
                            continue
                        
                        # è·³è¿‡éçƒ­ç‚¹é¡¹
                        if any(x in text for x in ['ç™»å½•', 'å…³äºæˆ‘ä»¬', 'Appä¸‹è½½']):
                            continue
                        
                        # æå–æ ‡é¢˜å’Œé“¾æ¥
                        title = ''
                        url = ''
                        
                        link = element.find('a')
                        if link:
                            title = link.get_text(strip=True)
                            url = link.get('href', '')
                            if url and not url.startswith(('http://', 'https://')):
                                url = self.base_url + url if url.startswith('/') else url
                        else:
                            # å¦‚æœæ²¡æœ‰é“¾æ¥ï¼Œä½¿ç”¨ç¬¬ä¸€è¡Œæ–‡æœ¬
                            lines = text.split('\n')
                            if lines:
                                title = lines[0].strip()
                        
                        if not title:
                            continue
                        
                        # æå–çƒ­åº¦å€¼
                        hot_value = None
                        hot_pattern = r'(\d+[kKmM]?)\s*(çƒ­åº¦|çƒ­|ğŸ”¥)'
                        match = re.search(hot_pattern, text)
                        if match:
                            hot_value = match.group(1)
                        
                        item = HotItem(
                            rank=idx,
                            title=title[:100],
                            url=url,
                            hot_value=hot_value
                        )
                        items.append(item)
                        
                    except Exception as e:
                        self._debug_log(f"è§£æå…ƒç´ å¤±è´¥: {e}")
                        continue
                
                if items:
                    break
        
        return items
    
    def _parse_zhihu(self, soup: BeautifulSoup) -> List[HotItem]:
        """è§£æçŸ¥ä¹çƒ­æ¦œ"""
        return self._parse_general(soup)  # æš‚æ—¶ä½¿ç”¨é€šç”¨è§£æ
    
    def _parse_baidu(self, soup: BeautifulSoup) -> List[HotItem]:
        """è§£æç™¾åº¦çƒ­ç‚¹"""
        return self._parse_general(soup)  # æš‚æ—¶ä½¿ç”¨é€šç”¨è§£æ
    
    def _parse_general(self, soup: BeautifulSoup) -> List[HotItem]:
        """é€šç”¨è§£ææ–¹æ³•"""
        items = []
        
        # æŸ¥æ‰¾æ‰€æœ‰è¡¨æ ¼
        tables = soup.find_all('table')
        for table_idx, table in enumerate(tables):
            rows = table.find_all('tr')
            
            for row_idx, row in enumerate(rows[1:51], 1):  # è·³è¿‡è¡¨å¤´
                try:
                    cols = row.find_all('td')
                    if len(cols) >= 2:
                        # æ’å
                        rank_text = cols[0].get_text(strip=True)
                        rank = row_idx
                        if rank_text.isdigit():
                            rank = int(rank_text)
                        
                        # æ ‡é¢˜å’Œé“¾æ¥
                        title_elem = cols[1].find('a')
                        if title_elem:
                            title = title_elem.get_text(strip=True)
                            url = title_elem.get('href', '')
                            
                            if title:
                                # å¤„ç†URL
                                if url and not url.startswith(('http://', 'https://')):
                                    url = self.base_url + url if url.startswith('/') else url
                                
                                # çƒ­åº¦å€¼
                                hot_value = None
                                if len(cols) >= 3:
                                    hot_value = cols[2].get_text(strip=True)
                                
                                item = HotItem(
                                    rank=rank,
                                    title=title[:100],
                                    url=url,
                                    hot_value=hot_value
                                )
                                items.append(item)
                except:
                    continue
        
        # å¦‚æœæ²¡æœ‰è¡¨æ ¼ï¼Œå°è¯•å…¶ä»–æ–¹å¼
        if not items:
            # æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥å’Œæ–‡æœ¬
            all_text = soup.get_text()
            lines = [line.strip() for line in all_text.split('\n') if line.strip()]
            
            for idx, line in enumerate(lines[:100], 1):
                if 10 < len(line) < 200:
                    # è·³è¿‡æ˜æ˜¾ä¸æ˜¯çƒ­ç‚¹çš„è¡Œ
                    if not any(x in line for x in ['script', 'function', 'var ', 'const ', 'ç™»å½•']):
                        item = HotItem(
                            rank=idx,
                            title=line[:80],
                            url='',
                            hot_value=None
                        )
                        items.append(item)
        
        return items